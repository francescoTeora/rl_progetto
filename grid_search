"""Grid Search per Hopper con Vanilla REINFORCE"""
import argparse
import gym
import torch
import numpy as np
import pandas as pd

from env.custom_hopper import *  # custom environment wrapper
from agent import Agent, Policy


def parse_args():
    parser = argparse.ArgumentParser(description="Grid search hyperparameters per Vanilla REINFORCE su MuJoCo Hopper")
    parser.add_argument('--env', default='CustomHopper-source-v0', type=str,
                        help="Nome dell'environment Gym")
    parser.add_argument('--n-episodes', default=5000, type=int,
                        help='Numero di episodi per ogni trial di training')
    parser.add_argument('--eval-window', default=100, type=int,
                        help='Numero di episodi finali per calcolare reward medio')
    parser.add_argument('--seeds', nargs='+', default=[0,1,2,3,4], type=int,
                        help='Lista di semi per ripetizioni')
    # Grid params
    parser.add_argument('--lrs', nargs='+', default=[1e-4,3e-4,1e-3], type=float,
                        help='Learning rates da provare')
    parser.add_argument('--gammas', nargs='+', default=[0.95,0.99], type=float,
                        help='Discount factors da provare')
    parser.add_argument('--batch-sizes', nargs='+', default=[1,5,10,30], type=int,
                        help='Numero di episodi per aggiornamento (batch size)')
    parser.add_argument('--device', default='cuda', type=str,
                        help='Device di training [cpu, cuda]')
    return parser.parse_args()


def train_episode(env, agent, max_steps=500):
    """
    Esegue un episodio e accumula transizioni nell'agente.
    Restituisce il reward totale dell'episodio.
    """
    state = env.reset()
    total_reward = 0.0
    done = False
    steps = 0
    while not done and steps < max_steps:
        action, log_prob = agent.get_action(state)
        next_state, reward, done, _ = env.step(action.detach().cpu().numpy())

        agent.store_outcome(state, next_state, log_prob, reward, done)
        state = next_state
        total_reward += reward
        steps += 1
    return total_reward


def run_grid_search(args):
    results = []
    for lr in args.lrs:
        for gamma in args.gammas:
            for batch_size in args.batch_sizes:
                for seed in args.seeds:
                    # Setup seed
                    torch.manual_seed(seed)
                    np.random.seed(seed)
                    env = gym.make(args.env)
                    env.seed(seed)

                    # Inizializza policy e agente (vanilla reinforce, no baseline)
                    obs_dim = env.observation_space.shape[-1]
                    act_dim = env.action_space.shape[-1]
                    policy = Policy(obs_dim, act_dim)
                    agent = Agent(policy,
                                  use_baseline=False,
                                  critic=False,
                                  device=args.device)
                    agent.optimizer = torch.optim.Adam(agent.policy.parameters(), lr=lr)
                    agent.gamma = gamma

                    # Training loop
                    rewards = []
                    for ep in range(1, args.n_episodes + 1):
                        ep_return = train_episode(env, agent)
                        rewards.append(ep_return)

                        # Update policy ogni batch_size episodi
                        if ep % batch_size == 0 and len(agent.rewards) > 0:
                            agent.update_policy()

                    # Valutazione
                    avg_last = np.mean(rewards[-args.eval_window:])
                    # Stampa risultati parziali per debug
                    print(f"lr: {lr}, gamma: {gamma}, batch_size: {batch_size}, seed: {seed}, avg_last_{args.eval_window}: {avg_last:.2f}")
                    results.append({
                        'lr': lr,
                        'gamma': gamma,
                        'batch_size': batch_size,
                        'seed': seed,
                        f'avg_last_{args.eval_window}': avg_last
                    })

                    env.close()
    # Analisi risultati
    df = pd.DataFrame(results)
    stats = df.groupby(['lr','gamma','batch_size'])[f'avg_last_{args.eval_window}'] \
             .agg(['mean','std']).reset_index()\
             .sort_values(by='mean', ascending=False)

    # Salvataggio
    out_csv = 'grid_search_results.csv'
    stats.to_csv(out_csv, index=False)
    print(f"Grid search completata. Risultati salvati in {out_csv}")
    print(stats.head(10))


def main():
    args = parse_args()
    total_trials = len(args.lrs) * len(args.gammas) * len(args.batch_sizes) * len(args.seeds)
    print(f"Avvio grid search su {args.env} con {total_trials} trial totali.")
    run_grid_search(args)


if __name__ == '__main__':
    main()
