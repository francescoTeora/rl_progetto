""" Grid Search for REINFORCE
evaluating in a naive way

- Mean of rewards of the last N episodes
- Trend of the rewards curve
- final standard deviation estimation
- 

"""
import argparse
import gym
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import linregress

from env.custom_hopper import *
from agent import Agent, Policy


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--env', default='CustomHopper-source-v0', type=str)
    parser.add_argument('--n-episodes', default=10000, type=int)
    parser.add_argument('--eval-window', default=500, type=int)
    parser.add_argument('--seeds', nargs='+', default=[0,1,2,3,4], type=int)
    parser.add_argument('--lrs', nargs='+', default=[1e-4, 1e-3], type=float)
    parser.add_argument('--gammas', nargs='+', default=[0.995, 0.99], type=float)
    parser.add_argument('--batch-sizes', nargs='+', default=[20, 30], type=int)
    parser.add_argument('--device', default='cuda', type=str)
    #parser.add_argument('--plot', action='store_true', help='Mostra i grafici dei reward')
    return parser.parse_args()


def train_episode(env, agent, max_steps=500):
    state = env.reset()
    done = False
    total_reward = 0.0
    steps = 0
    while not done and steps < max_steps:
        action, log_prob = agent.get_action(state)
        next_state, reward, done, _ = env.step(action.detach().cpu().numpy())
        agent.store_outcome(state, next_state, log_prob, reward, done)
        state = next_state
        total_reward += reward
        steps += 1
    return total_reward


def evaluate_rewards(rewards, eval_window):
    final_rewards = rewards[-eval_window:]
    avg = np.mean(final_rewards)
    std = np.std(final_rewards)
    trend_slope, _, _, pval, _ = linregress(range(len(final_rewards)), final_rewards)
    return avg, std, trend_slope, pval


def plot_rewards_curve(rewards, title):
    plt.figure(figsize=(10, 5))
    plt.plot(rewards, label='Episode Reward')
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.title(title)
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()


def run_grid_search(args):
    results = []
    for lr in args.lrs:
        for gamma in args.gammas:
            for batch_size in args.batch_sizes:
                for seed in args.seeds:
                    torch.manual_seed(seed)
                    np.random.seed(seed)
                    env = gym.make(args.env)
                    env.seed(seed)

                    obs_dim = env.observation_space.shape[-1]
                    act_dim = env.action_space.shape[-1]
                    policy = Policy(obs_dim, act_dim)
                    agent = Agent(policy, use_baseline=False, critic=False, device=args.device)
                    agent.optimizer = torch.optim.Adam(agent.policy.parameters(), lr=lr)
                    agent.gamma = gamma

                    rewards = []
                    for ep in range(1, args.n_episodes + 1):
                        r = train_episode(env, agent)
                        rewards.append(r)
                        if ep % batch_size == 0 and len(agent.rewards) > 0:
                            agent.update_policy()

                    avg, std, slope, pval = evaluate_rewards(rewards, args.eval_window)

                    print(f"lr={lr:.0e}, gamma={gamma}, batch={batch_size}, seed={seed} => avg={avg:.1f}, slope={slope:.2f}, p={pval:.2f}")

                    

                    results.append({
                        'lr': lr,
                        'gamma': gamma,
                        'batch_size': batch_size,
                        'seed': seed,
                        f'avg_last_{args.eval_window}': avg,
                        'std': std,
                        'reward_slope': slope,
                        'slope_pval': pval
                    })
                    env.close()

    df = pd.DataFrame(results)
    summary = df.groupby(['lr','gamma','batch_size']).agg({
        f'avg_last_{args.eval_window}': ['mean','std'],
        'reward_slope': 'mean',
        'std': 'mean'
    }).reset_index()

    summary.columns = ['lr','gamma','batch_size','avg_reward_mean','avg_reward_std','reward_slope','reward_std']
    summary = summary.sort_values(by='avg_reward_mean', ascending=False)
    summary.to_csv("grid_search_light_results.csv", index=False)
    print("Best results by mean:")
    print(summary.head(10))


def main():
    args = parse_args()
    print(f"grid search on {args.env}, episodes={args.n_episodes}, seeds={len(args.seeds)}")
    run_grid_search(args)


if __name__ == '__main__':
    main()
