import gym
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from env.custom_hopper import *

class CurriculumDomainRandomizationAdaptive(gym.Wrapper):
    """
    Versione minimale di Adaptive CDR
    """
    def __init__(self, env, min_range=0.01, max_range=0.10):
        super().__init__(env)
        self.min_range = min_range
        self.max_range = max_range
        self.current_range = min_range
        
        # Tracking semplice con lista normale
        self.episode_rewards = []
        self.current_episode_reward = 0
        self.reset_count = 0
        
    def reset(self, **kwargs):
        self.reset_count += 1
        
        # Salva reward dell'episodio precedente
        if self.reset_count > 1:
            self.episode_rewards.append(self.current_episode_reward)
            
            # Mantieni solo gli ultimi 1000 episodi
            if len(self.episode_rewards) > 1000:
                self.episode_rewards.pop(0)
            
            # Controlla se puÃ² aumentare difficoltÃ  ogni 100 episodi
            if len(self.episode_rewards) >= 500 and self.reset_count % 100 == 0:
                self._check_difficulty_increase()
        
        # Reset per nuovo episodio
        self.current_episode_reward = 0
        
        # Applica randomizzazione corrente
        if hasattr(self.env, 'set_random_parameters'):
            self.env.set_random_parameters(self.current_range)
        
        obs = self.env.reset(**kwargs)
        return obs[0] if isinstance(obs, tuple) else obs
    
    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        self.current_episode_reward += reward
        return obs, reward, done, info
    
    def _check_difficulty_increase(self):
        """
        Logica semplice: se la media degli ultimi 500 episodi Ã¨ migliorata,
        aumenta la difficoltÃ 
        """
        if self.current_range >= self.max_range:
            return  # GiÃ  al massimo
        
        # Prendi ultimi 500 episodi
        recent_rewards = self.episode_rewards[-500:]
        recent_avg = np.mean(recent_rewards)
        
        # Se abbiamo almeno 1000 episodi, confronta con i 500 precedenti
        if len(self.episode_rewards) >= 1000:
            previous_rewards = self.episode_rewards[-1000:-500]
            previous_avg = np.mean(previous_rewards)
            
            # Se c'Ã¨ miglioramento, aumenta difficoltÃ 
            if recent_avg > previous_avg:
                old_range = self.current_range
                self.current_range = min(self.max_range, self.current_range + 0.02)
                
                print(f"ðŸŽ¯ Difficulty UP! Range: {old_range:.3f} â†’ {self.current_range:.3f}")
                print(f"   Performance: {previous_avg:.1f} â†’ {recent_avg:.1f}")

def make_env():
    """Factory per creare ambiente con CDR"""
    def _init():
        env = gym.make('CustomHopper-source-v0')
        env = CurriculumDomainRandomizationAdaptive(env)
        return env
    return _init

def train_simple_adaptive():
    """Training con CDR adattivo semplice"""
    print("Training with Simple Adaptive CDR...")
    
    # Ambiente di training
    train_env = DummyVecEnv([make_env()])
    total_timesteps =500_000
    # Ambiente di test
    target_env = DummyVecEnv([lambda: gym.make('CustomHopper-target-v0')])
    def lr_schedule(progress_remaining):
        current_step = int((1 - progress_remaining) * total_timesteps)
        factor = current_step //352356  # dimezza ogni 352356 step
        return 1e-3 * (0.5 ** factor)
    # Modello PPO
    model = PPO(
            policy="MlpPolicy",  # Policy network architecture
            env=train_env,       # Training environment
            learning_rate=lr_schedule,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            clip_range=0.2,
            ent_coef=0.1,
            normalize_advantage=True,
            tensorboard_log=f"./curriculum_bohhhhhh_tensorboard/",
            verbose=1
        )
    
    # Addestra
    model.learn(total_timesteps)
    
    # Valuta
    from stable_baselines3.common.evaluation import evaluate_policy
    mean_reward, std_reward = evaluate_policy(model, target_env, n_eval_episodes=10)
    
    print(f"\nResults: {mean_reward:.2f} Â± {std_reward:.2f}")
    
    # Statistiche finali
    wrapper = train_env.envs[0]
    print(f"Final range: {wrapper.current_range:.3f}")
    print(f"Episodes: {wrapper.reset_count}")
    
    train_env.close()
    target_env.close()
    
    return mean_reward

if __name__ == "__main__":
    train_simple_adaptive()
